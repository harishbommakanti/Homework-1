{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='notebook_header'><b>CS 309 - Robot Learning</b></p>\n",
    "<p class='notebook_header'>Homework 1</p>\n",
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 1: Linear Algebra</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following matrix/vector functions using NumPy operations.\n",
    "\n",
    "If the function's operation isn't possible for matrix or vector inputs, return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return np.add(a,b);\n",
    "\n",
    "def subtract(a, b):\n",
    "    return np.subtract(a, b);\n",
    "\n",
    "def multiply(a, b):\n",
    "    return np.matmul(a,b);\n",
    "\n",
    "def divide(a, b):\n",
    "    return None;\n",
    "\n",
    "def transpose(a):\n",
    "    return a.T;\n",
    "\n",
    "def two_norm(a):\n",
    "    return np.linalg.norm(a, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using your code from above, solve the following equations. If an operation isn't possible, put None or comment with \"Not Possible\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\begin{bmatrix} 2 \\\\ 3 \\\\ 9 \\end{bmatrix}, \\:\n",
    "v = \\begin{bmatrix} -2 \\\\ 1 \\\\ 8 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [9]\n",
    "])\n",
    "\n",
    "v = np.array([\n",
    "    [-2],\n",
    "    [1],\n",
    "    [8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u + v = \\begin{bmatrix} 0 \\\\ 4 \\\\ 17 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u - v = \\begin{bmatrix} 4 \\\\ 2 \\\\ 1 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v = \\text{Not Possible} $$  \n",
    "\n",
    "$$ u \\div v = \\text{Not Possible} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_plus_v = add(u,v)\n",
    "\n",
    "u_minus_v = subtract(u,v)\n",
    "\n",
    "u_mult_v = None # not possible, dimensions of u and v are both (3,1) so multiplication can't happen\n",
    "\n",
    "u_div_v = divide(u,v) # this returns None anyways, not possible since dividing matrices isn't allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u^{\\;T} * v = \\begin{bmatrix} 71 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v^{\\;T} = \\begin{bmatrix} -4 & 2 & 16 \\\\ -6 & 3 & 24 \\\\ -18 & 9 & 72 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u^{\\;T} * u = \\begin{bmatrix} 94 \\end{bmatrix} $$  \n",
    "\n",
    "$$ \\left \\| u \\right \\|_{2}^{2} = 94.0 $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71]]\n",
      "[[ -4   2  16]\n",
      " [ -6   3  24]\n",
      " [-18   9  72]]\n",
      "[[94]]\n",
      "94.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_transpose_v = multiply(transpose(u), v)\n",
    "print(u_transpose_v)\n",
    "\n",
    "u_v_tranpose = multiply(u, transpose(v))\n",
    "print(u_v_tranpose)\n",
    "\n",
    "u_tranpose_u = multiply(transpose(u), u)\n",
    "print(u_tranpose_u)\n",
    "\n",
    "two_norm_u = two_norm(u) ** 2\n",
    "print(two_norm_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix} 1 & 6 & 5\\\\ 0 & -4 & -1\\\\ 7 & 2 & 3 \\end{bmatrix}, \\: \n",
    "B = \\begin{bmatrix} 3 & 1 & 1\\\\ 4 & -1 & 7\\\\ 7 & 0 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A + B = \\begin{bmatrix} 4 & 7 & 6 \\\\ 4 & -5 & 6 \\\\ 14 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A - B = \\begin{bmatrix} -2 & 5 & 4 \\\\ -4 & -3 & -8 \\\\ 0 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A * B = \\begin{bmatrix} 62 & -5 & 43 \\\\ -23 & 4 & -28 \\\\ 50 & 5 & 21 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A \\div B = \\text{Not Possible} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7  6]\n",
      " [ 4 -5  6]\n",
      " [14  2  3]]\n",
      "[[-2  5  4]\n",
      " [-4 -3 -8]\n",
      " [ 0  2  3]]\n",
      "[[ 62  -5  43]\n",
      " [-23   4 -28]\n",
      " [ 50   5  21]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "a = np.array([\n",
    "    [1, 6, 5],\n",
    "    [0, -4, -1],\n",
    "    [7, 2, 3]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [3, 1, 1],\n",
    "    [4, -1, 7],\n",
    "    [7, 0, 0]\n",
    "])\n",
    "\n",
    "a_plus_b = add(a,b)\n",
    "print(a_plus_b)\n",
    "\n",
    "a_minus_b = subtract(a,b)\n",
    "print(a_minus_b)\n",
    "\n",
    "a_mult_b = multiply(a,b)\n",
    "print(a_mult_b)\n",
    "\n",
    "a_div_b = divide(a,b)\n",
    "print(a_div_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C = \\begin{bmatrix} 5 & 1 \\\\ -1 & 7 \\\\ 3 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[5, 1],\n",
    "              [-1, 7],\n",
    "              [3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right Pseudo Inverse of C:\n",
    "> $$\n",
    "\\begin{align}\n",
    "    &= C^T(C C^T)^{-1}\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.3125 & -0.0078125 & 0.25 \\\\\n",
    "    0.05859375 & 0.14208984 & 0.0390625\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "Left Pseudo Inverse of C:\n",
    ">$$\n",
    "\\begin{align}\n",
    "&= (C^T C)^{-1} C^T \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0.1443299 & -0.02061856 & 0.08591065 \\\\\n",
    "0.0257732 & 0.13917526 & 0.00343643\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3125     -0.0078125   0.25      ]\n",
      " [ 0.05859375  0.14208984  0.0390625 ]]\n",
      "[[ 0.1443299  -0.02061856  0.08591065]\n",
      " [ 0.0257732   0.13917526  0.00343643]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "right_pinv = multiply(transpose(c), np.linalg.inv(multiply(c, transpose(c))))\n",
    "print(right_pinv)\n",
    "\n",
    "left_pinv = multiply(np.linalg.inv(multiply(transpose(c), c)), transpose(c))\n",
    "print(left_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 2: Regression</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ordinary Least Squares below. \n",
    "\n",
    "> $$\n",
    "\\hat{\\Theta} = ((X^T \\ X)^{-1} \\ X^T) \\ Y \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** Ordinary Least Squares in terms of what it optimizes.\n",
    "\n",
    "> Ordinary Least Squares (OLS) tries to find the line of best fit (in any dimension) in a given dataset. It does this by trying to minimize the distance between the predicted value in the line of best fit and the actual, given value, for each point in the dataset. The goal is to find a $\\hat{\\Theta}$ representing the optimum slopes and intercepts for the function to represent the line of best fit. Here, $\\hat{\\Theta}$ will represent the optimum line of best fit and $\\underline{\\Theta}$ will represent an imperfect line of best fit.\n",
    "\n",
    "> In mathematical terms, the for every data point $i$, the predicted value is $x_i^T \\underline{\\Theta}$ and the ground truth value is $y_i$. To this minimization goal, a few modifications are added. The goal is squared to ensure that the magnitude of the error is being minimized. It is multiplied by $\\frac{1}{N}$ to aid in the math when finding the optimal $\\hat{\\Theta}$.\n",
    "\n",
    "> Then, summing the minimization goal over all datapoints and creating a function $argmin(\\underline{\\Theta})$ to accept the argument $\\underline{\\Theta}$ and return the optimum $\\hat{\\Theta}$ that minimizes the optimization goal, we have:\n",
    "\n",
    "$$\\hat{\\Theta} = argmin(\\underline{\\Theta})\\Bigg{(} \\ \\frac{1}{N} \\ \\sum_{i}^{N} [X_i^T \\underline{\\Theta} - Y_i]^2\\Bigg{)}$$\n",
    "\n",
    "> After doing some calculus, this simplifies to:\n",
    "$$\n",
    "\\hat{\\Theta} = ((X^T \\ X)^{-1} \\ X^T) \\ Y \\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change this cell!\n",
    "# Load in the data about the study on students\n",
    "train = np.loadtxt('train.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = train.T\n",
    "X_train = np.array([x_0, x_1, x_2]).T\n",
    "Y_train = np.expand_dims(y, 1)\n",
    "\n",
    "test = np.loadtxt('test.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = test.T\n",
    "X_test = np.array([x_0, x_1, x_2]).T\n",
    "Y_test = np.expand_dims(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an imaginary study done on 101 students at Crest University. The study surveyed students for the amount they have spent on electronics, books, pencils, and foods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the **amount students spend on electronics ($Y$)** is linearly related to the **amount they spend on books ($ X_{0} $), pencils ($ X_{1}$)**, and **food ($ X_{2}$)**, \n",
    "**implement** the Ordinary Least Squares method to model this regression problem.\n",
    "\n",
    "The data is read in from the previous cell code. **X_train** has the input features, while **Y_train** has corresponding target outputs.\n",
    "\n",
    "After finding a solution, try to measure the error between your predictions and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for training set:  3532.974816901931\n",
      "Error for testing set:  18157.1846126346\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for OLS here. DO NOT use any other libraries in your first implementation.\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "def OLS(X, y):\n",
    "    pseudo_inv = multiply(np.linalg.inv(multiply(X.T, X)), X.T)\n",
    "    theta = multiply(pseudo_inv, y)\n",
    "    return theta\n",
    "\n",
    "# multiply X and theta to get the prediction\n",
    "def get_prediction(X, theta):\n",
    "    assert(X.shape[1] == theta.shape[0])\n",
    "    return multiply(X, theta)\n",
    "\n",
    "# this is the OLS minimization objective as shown above,\n",
    "# (1/n) * sum(...)\n",
    "def get_OLS_error(prediction, ground_truth):\n",
    "    assert(prediction.shape == ground_truth.shape)\n",
    "    \n",
    "    n = prediction.shape[0]\n",
    "    differences = (prediction - ground_truth).flatten() ** 2\n",
    "    return np.sum(differences) / n\n",
    "\n",
    "# perform the regression on the training data\n",
    "theta = OLS(X_train,Y_train)\n",
    "# print(theta)\n",
    "prediction = get_prediction(X_train, theta)\n",
    "error = get_OLS_error(prediction, Y_train)\n",
    "print(\"Error for training set: \", error)\n",
    "\n",
    "# perform the regression on the testing data\n",
    "theta = OLS(X_test, Y_test)\n",
    "# print(theta)\n",
    "prediction = get_prediction(X_test, theta)\n",
    "error = get_OLS_error(prediction, Y_test)\n",
    "print(\"Error for testing set: \", error)\n",
    "\n",
    "\n",
    "# Both errors are pretty large, which shows us that OLS does not\n",
    "# perform well on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what collinearity is.\n",
    "\n",
    "> Collinearity is when some of the features in the training set are dependent. This mathematically means that one feature could be represented as a multiple of the other. What this means in the scope of OLS is that there could be a line of solutions rather than a single solution, which defeats the point of regression since a single solution is wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ridge Regression below. \n",
    "\n",
    "> Using $\\Phi$ as the feature matrix of $X$, we have:\n",
    "> $$\\hat{\\Theta} = (\\Phi^T\\Phi + \\lambda \\ \\text{I})^{-1} \\Phi^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what the purpose ridge regression and its advantages and disadvantages over OLS.\n",
    "\n",
    "> Ridge regression is meant to combat the problem of having dependent features produce a line of solutions rather than a single solution. The way it does this is by adding a regulator term in the minimization objective:\n",
    "\n",
    "> Ridge regression minimization objective: $\\hat{\\Theta} = argmin(\\underline{\\Theta}) \\frac{1}{2} \\Big{(} || y - \\Phi \\underline{\\Theta}||_2^2 + \\lambda ||\\underline{\\Theta}||_2^2\\Big{)} $\n",
    "\n",
    "> Where the $\\lambda ||\\underline{\\Theta}||_2^2$ is the regularization term. The $\\lambda$ is a hyperparameter. Using calculus similar to the math used to derive OLS, we arrive at the equationa above for the optimal $\\hat{\\Theta}$.\n",
    "\n",
    "\n",
    "> The advantages of ridge regression over OLS are that since a regularization factor is added to the minimization objective, a single solution is guaranteed. This means that the single, optimal solution was reached rather than one of the many, possibly infinite solutions, that would've seemed like the optimal solution in OLS. Additionally, using ridge regression allows you to examine the data in terms of features rather than the specific numeric values which may provide further insight on the data.\n",
    "\n",
    "> The disadvantages of ridge regression is that there is a hyperparameter to tune, $\\lambda$, but since there is only one parameter, it is not too much of a disadvantage. Other than this, there is not really much disadvantage; ridge regression is mostly a step up from OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** your regression model with ridge regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge=0.3\n",
      "Error for training set:  1141.503832815676\n",
      "Error for testing set:  1177.0187352657965\n",
      "\n",
      "ridge=0.1\n",
      "Error for training set:  1141.5038390476188\n",
      "Error for testing set:  1177.0187479999522\n",
      "\n",
      "ridge=0.01\n",
      "Error for training set:  1141.5038457631474\n",
      "Error for testing set:  1177.019314104384\n",
      "\n",
      "ridge=0.03\n",
      "Error for training set:  1141.5038418034155\n",
      "Error for testing set:  1177.0188078334304\n",
      "\n",
      "ridge=0.001\n",
      "Error for training set:  1141.5046272568266\n",
      "Error for testing set:  1177.0762787791\n",
      "\n",
      "ridge=0.003\n",
      "Error for training set:  1141.5039389728722\n",
      "Error for testing set:  1177.0250908037758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Ridge Regression here. DO NOT use any other libraries\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "# analytical solution to RR\n",
    "def RR(X, y, ridge=0.001):\n",
    "    phi_t_phi = multiply(X.T, X)\n",
    "    regularizer = ridge * np.identity(phi_t_phi.shape[0])\n",
    "    phi_t_y = multiply(X.T, y)\n",
    "    inverse_part = np.linalg.inv(add(phi_t_phi, regularizer))\n",
    "    theta = multiply(inverse_part, phi_t_y)\n",
    "    return theta\n",
    "\n",
    "# minimization objective of RR as shown above\n",
    "def get_RR_error(prediction, theta, ground_truth, ridge):\n",
    "    assert(prediction.shape == ground_truth.shape)\n",
    "    \n",
    "    n = prediction.shape[0]\n",
    "    differences = two_norm((ground_truth - prediction).flatten()) ** 2\n",
    "    regularizer = two_norm(theta.flatten()) ** 2\n",
    "    return (differences + regularizer) / 2\n",
    "    \n",
    "\n",
    "# using the get_prediction and get_error functions from above:\n",
    "\n",
    "# check against training data\n",
    "for ridge in [0.3, 0.1, 0.01, 0.03, 0.001, 0.003]:\n",
    "    print(f\"ridge={ridge}\")\n",
    "    theta = RR(X_train,Y_train, ridge)\n",
    "#     print(theta)\n",
    "    pred = get_prediction(X_train, theta)\n",
    "    error = get_RR_error(pred, theta, Y_train, ridge)\n",
    "    print(\"Error for training set: \", error)\n",
    "\n",
    "    # test your model on testing data\n",
    "    theta = RR(X_test, Y_test, ridge)\n",
    "    pred = get_prediction(X_test, theta)\n",
    "    error = get_RR_error(pred, theta, Y_test, ridge)\n",
    "    print(\"Error for testing set: \", error)\n",
    "    print()\n",
    "\n",
    "\n",
    "# We can see that the errors here are much smaller than the\n",
    "# error for OLS. additionally, the errors are constant between\n",
    "# the training and testing sets, as well as between different\n",
    "# ridge values, which hint at a more consistent\n",
    "# model less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences ridge regression created for theta compared to OLS, and why these differences even existed. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "> By using `print(theta)` (print statements commented out above), it seems that the `theta` created by the OLS method had weights that were sometimes huge, in the thousands and ten thousands, whereas Ridge Regression produced theta values which always had components which were very small, mostly under 5. Because of the advantage RR gives over OLS, the first thought that comes to mind is that the differences might have been created by collinear features. Indeed, in the dataset, the amount of money students spend on books and pencils may be correlated, leading to RR having a much lower average error. The reason that Ridge had smaller weights overall was probably because, using the regularization term against the `argmin` function, it punishes for large weights, so it desires a smaller $\\hat{\\Theta}$.\n",
    "\n",
    "> There are differences over each ridge value but since they are so minimal, possibly due to the relative closeness of the chosen ridge values, the errors don't change too much. If the ridge values are set at something like 3000 however, the ridge values alter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other regularizers other than ridge regression, such as LASSO. **Explain** the differences between LASSO and Ridge Regression and how it changes the solution mathematically.\n",
    "\n",
    "> LASSO regression adds the regularizer term $|\\underline{\\Theta}|$ instead of adding the regularizer term $||\\underline{\\Theta}||_2^2$ (punishes for the absolute value only instead of the sum of squares). This allows some coefficients to be 0 which means that some features may be taken out of the picture all together, resulting in a simpler model of the problem. Both Ridge and LASSO regression can be used to tackle the problem of collinear factors though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.3\n",
      "[ 2.99055607  2.01740188 -0.00363305]\n",
      "Error for training set:  3599.4252615626174\n",
      "[2.99269029e+00 1.99687990e+00 2.28805633e-03]\n",
      "Error for testing set:  1952.750975291754\n",
      "\n",
      "alpha=0.1\n",
      "[ 2.99085162  2.01755857 -0.00366872]\n",
      "Error for training set:  3593.782307860037\n",
      "[2.99302044e+00 1.99709802e+00 2.24353785e-03]\n",
      "Error for testing set:  1949.116657317834\n",
      "\n",
      "alpha=0.01\n",
      "[ 2.99098462  2.01762907 -0.00368477]\n",
      "Error for training set:  3591.246399906312\n",
      "[2.99316901e+00 1.99719617e+00 2.22350453e-03]\n",
      "Error for testing set:  1947.4853188915677\n",
      "\n",
      "alpha=0.03\n",
      "[ 2.99095507  2.01761341 -0.00368121]\n",
      "Error for training set:  3591.8097514938345\n",
      "[2.99313600e+00 1.99717436e+00 2.22795638e-03]\n",
      "Error for testing set:  1947.8476183684038\n",
      "\n",
      "alpha=0.001\n",
      "[ 2.99099792  2.01763613 -0.00368638]\n",
      "Error for training set:  3590.99292590406\n",
      "[2.99318387e+00 1.99720599e+00 2.22150120e-03]\n",
      "Error for testing set:  1947.3223251735978\n",
      "\n",
      "alpha=0.003\n",
      "[ 2.99099497  2.01763456 -0.00368602]\n",
      "Error for training set:  3591.0492516249587\n",
      "[2.99318057e+00 1.99720381e+00 2.22194639e-03]\n",
      "Error for testing set:  1947.3585437980728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for LASSO here\n",
    "# TODO: Plot your regression line over the input points. \n",
    "\n",
    "# Use sklearn.linear_model for the LASSO and Elastic Net implementations\n",
    "# !pip install sklearn\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "def LASSO(X, y, alpha=.001):\n",
    "    classifier = linear_model.Lasso(alpha)\n",
    "    classifier.fit(X, y)\n",
    "    return classifier.coef_\n",
    "\n",
    "# minimization objective of Lasso as shown above\n",
    "def get_Lasso_error(prediction, theta, ground_truth):    \n",
    "    n = prediction.shape[0]\n",
    "    differences = two_norm((ground_truth - prediction)) ** 2\n",
    "    regularizer = np.sum(np.abs(theta))\n",
    "    return (differences + regularizer) / 2\n",
    "\n",
    "# TODO: check against training data\n",
    "for alpha in [0.3, 0.1, 0.01, 0.03, 0.001, 0.003]:\n",
    "    print(f\"alpha={alpha}\")\n",
    "    theta = LASSO(X_train, Y_train, alpha)\n",
    "    print(theta)\n",
    "    prediction = get_prediction(X_train, theta)\n",
    "    error = get_Lasso_error(prediction, theta, Y_train.flatten())\n",
    "    print(\"Error for training set: \", error)\n",
    "\n",
    "\n",
    "    # TODO: check against testing data\n",
    "    theta = LASSO(X_test, Y_test, alpha)\n",
    "    print(theta)\n",
    "    prediction = get_prediction(X_test, theta)\n",
    "    error = get_Lasso_error(prediction, theta, Y_test.flatten())\n",
    "    print(\"Error for testing set: \", error)\n",
    "    print()\n",
    "    # print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the effect elastic nets had on your values for theta compared to OLS. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "> We can see that LASSO is somewhat of a middle ground between OLS and Ridge regression in terms of the errors seen and the thetas. The `theta` produced by OLS was large and non-optimal, while the theta produced by LASSO were smaller. The thetas produced by LASSO weren't as small as prodced by Ridge regression though which makes sense, as Ridge regression punishes the extreme values more. The error produced by LASSO is also midway between the errors produced by OLS and Ridge: The errors produced by OLS were above 3500 and were not consistent. In LASSO, the errors are lower, looking to average at error 2500. Of course, ridge regression went further rand minimized the error to only around 1200.\n",
    "\n",
    "> Changing the alpha hyperparameter seem to alter the error a little more than the ridge regression: small changes between small numbers make the change in errors more noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences between LASSO, Ridge Regression and Elastic Nets and how it changes the solution mathematically.\n",
    "\n",
    "> * LASSO regression adds a single regularization term, the 1 norm of `theta`.\n",
    "> * Ridge regression adds a single regularization term, the 2 norm of `theta`.\n",
    "> * Elastic Nets add two regularization terms, both the 1 norm of `theta` and the 2 norm of `theta`.\n",
    "\n",
    "> Both Elastic Nets are a sort of middle ground betwen LASSO and Ridge regression, and tries to acommodate both punishment terms. Mathematically, the differences between the addition of the term(s) in `argmin` determines what in the weights are prioritized. Since LASSO and Elastic Nets both use the 1 norm regularization term, they both tend to get higher weights down towards 0. Ridge regression doesn't specifically drive weights towards 0 but just penalizes huge weights and tries to get high weights to low weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute value differences in the training set: \n",
      "[[ 11.21043291 270.27386981 463.57216973 ... 381.94779179 276.02206778\n",
      "   50.29429705]\n",
      " [256.64783629   2.41560061 195.71390053 ... 114.08952259   8.16379858\n",
      "  217.56397215]\n",
      " [451.44614149 192.38270459   0.91559533 ...  80.70878261 186.63450662\n",
      "  412.36227735]\n",
      " ...\n",
      " [368.85580749 109.79237059  83.50592933 ...   1.88155139 104.04417262\n",
      "  329.77194335]\n",
      " [260.35610179   1.29266489 192.00563503 ... 110.38125709   4.45553308\n",
      "  221.27223765]\n",
      " [ 27.61952709 231.44390981 424.74220973 ... 343.11783179 237.19210778\n",
      "   11.46433705]]\n",
      "\n",
      "Absolute value differences in the testing set: \n",
      "[[ 5.71715791 10.94636928 32.34176782 ... 54.54751277 60.53638314\n",
      "  17.26011007]\n",
      " [19.05229691  2.38876972 19.00662882 ... 67.88265177 73.87152214\n",
      "   3.92497107]\n",
      " [41.05823891 24.39471172  2.99931318 ... 89.88859377 95.87746414\n",
      "  18.08097093]\n",
      " ...\n",
      " [47.42757409 64.09110128 85.48649982 ...  1.40278077  7.39165114\n",
      "  70.40484207]\n",
      " [47.98624109 64.64976828 86.04516682 ...  0.84411377  6.83298414\n",
      "  70.96350907]\n",
      " [29.41050591 12.74697872  8.64841982 ... 78.24086077 84.22973114\n",
      "   6.43323793]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Elastic Nets here. You can use a library such as scipy\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "def EN(X,y):\n",
    "    classifier = linear_model.ElasticNet(random_state=0, max_iter=10000)\n",
    "    classifier.fit(X, y)\n",
    "    return classifier.coef_\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = EN(X_train, Y_train)\n",
    "prediction = get_prediction(X_train, theta)\n",
    "print(\"Absolute value differences in the training set: \")\n",
    "print(np.abs(prediction - Y_train))\n",
    "# print(theta)\n",
    "\n",
    "# TODO: check against testing data\n",
    "theta = EN(X_test, Y_test)\n",
    "prediction = get_prediction(X_test, theta)\n",
    "print(\"\\nAbsolute value differences in the testing set: \")\n",
    "print(np.abs(prediction - Y_test))\n",
    "# print(theta)\n",
    "\n",
    "\n",
    "# From this, it seems like Elastic Net truly does try to compensate\n",
    "# between LASSO and ridge. The thetas observed have 0's, like LASSO\n",
    "# prefers, and the errors between the predicted and ground truth values\n",
    "# are low, but not as low as ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is the purpose of a regularizer?\n",
    "\n",
    "> To reduce overfitting the model against the training set too much, and to reduce the effect that dependent features in the incoming data have on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give two examples where a regularizer would give more robust models.\n",
    "\n",
    "> A regularizer would give more robust models if a prediction is desired but the input data has collinear features. Here are two examples where this is the case:\n",
    "> 1. The number of colleges a students is admitted to based on factors like SAT score, scores in school, and number of extracurricular activities. Clearly a student with a higher number of one of those numbers would be likely to have a higher number of another factor: a student who scores higher on the SAT is probably more likely to have higher grades in school.\n",
    "> 2. Predicting the temperature of the weather based on factors like recent wind speed, precipitation levels, and the current season. Wind speed and precipitation levels are not independent: if there was a storm recently, there might have been high wind speed and high precipitation levels. Additionally, these two are not independent of the current season, since seasons like summer may have very low levels of wind speed and precipitation levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain with reference to the dataset why a regularizer achieved better performance than OLS.\n",
    "\n",
    "> Here specifically, the factors $X_0, X_1,$ and $X_2$ were probably dependent. $X_0$ and $X_1$ can clearly be seen as dependent, since a student who spends more on books is more likely to spend money on pencils. Additionally, all 3 may be dependent as a whole: a student with more money in general may be more likely to spend money on books, pencils, and food, than someone without much money at all. Therefore, adding a regularizer to reduce the effect that dependent features have on the prediction increased the performance of the models astronomically, as has been shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature transformation to fit a line to the curve generated from the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('feature_transform.csv', delimiter=',')\n",
    "y = X[:,2].reshape(500, 1)\n",
    "X = X[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Write the lambda function phi which will transform X\n",
    "# TODO: Plot the transformation and the resulting line after transforming\n",
    "\n",
    "phi = \"fill in with lambda function\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,0],y,color=(0.0,0.5,0.0))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what you learned about polynomial regression and explain what is happening to the model as you increase the degrees. Run the cell below and use the slider to help you.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cb8cb92395a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT ALTER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipywidgets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minteract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# DO NOT ALTER\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "data = pd.concat([data,pd.Series(boston.target,name='MEDV')],axis=1)\n",
    "\n",
    "X = data[['LSTAT']].values\n",
    "y = data['MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "temp = pd.DataFrame({'x':x_train.reshape(1, 354)[0], 'y':y_train})\n",
    "temp = temp.sort_values('x')\n",
    "x_train = temp['x'].values.reshape(354,1)\n",
    "y_train = temp['y'].values\n",
    "\n",
    "temp = pd.DataFrame({'x':x_test.reshape(1, 152)[0], 'y':y_test})\n",
    "temp = temp.sort_values('x')\n",
    "x_test = temp['x'].values.reshape(152,1)\n",
    "y_test = temp['y'].values\n",
    "\n",
    "def f(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(x_train,y_train)\n",
    "    y_plot = model.predict(x_test)\n",
    "    \n",
    "    plt.scatter(x_train, y_train, s=10, color='red', alpha=.3)\n",
    "    plt.scatter(x_test, y_test, s=10)\n",
    "\n",
    "    test_sr = (y_test - y_plot)**2\n",
    "    test_ssr = test_sr.sum()\n",
    "    test_asr = test_ssr/len(test_sr)\n",
    "    \n",
    "    y_plot_train = model.predict(x_train)\n",
    "    train_sr = (y_train - y_plot_train)**2\n",
    "    train_ssr = train_sr.sum()\n",
    "    train_asr = train_ssr/len(train_sr)\n",
    "    \n",
    "    plt.plot(x_test, y_plot, label=\"degree %d\" % degree + '; Test Error: %.2f' % test_asr + '; Train Error: %.2f' % train_asr, color='green')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "interact(f, degree = widgets.IntSlider(min=1, max=20, step=1, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
