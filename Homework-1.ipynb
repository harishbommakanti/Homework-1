{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='notebook_header'><b>CS 309 - Robot Learning</b></p>\n",
    "<p class='notebook_header'>Homework 1</p>\n",
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 1: Linear Algebra</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following matrix/vector functions using NumPy operations.\n",
    "\n",
    "If the function's operation isn't possible for matrix or vector inputs, return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return np.add(a,b);\n",
    "\n",
    "def subtract(a, b):\n",
    "    return np.subtract(a, b);\n",
    "\n",
    "def multiply(a, b):\n",
    "    return np.matmul(a,b);\n",
    "\n",
    "def divide(a, b):\n",
    "    return None;\n",
    "\n",
    "def transpose(a):\n",
    "    return a.T;\n",
    "\n",
    "def two_norm(a):\n",
    "    return np.linalg.norm(a, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using your code from above, solve the following equations. If an operation isn't possible, put None or comment with \"Not Possible\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\begin{bmatrix} 2 \\\\ 3 \\\\ 9 \\end{bmatrix}, \\:\n",
    "v = \\begin{bmatrix} -2 \\\\ 1 \\\\ 8 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [9]\n",
    "])\n",
    "\n",
    "v = np.array([\n",
    "    [-2],\n",
    "    [1],\n",
    "    [8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u + v = \\begin{bmatrix} 0 \\\\ 4 \\\\ 17 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u - v = \\begin{bmatrix} 4 \\\\ 2 \\\\ 1 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v = \\text{Not Possible} $$  \n",
    "\n",
    "$$ u \\div v = \\text{Not Possible} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_plus_v = add(u,v)\n",
    "\n",
    "u_minus_v = subtract(u,v)\n",
    "\n",
    "u_mult_v = None # not possible, dimensions of u and v are both (3,1) so multiplication can't happen\n",
    "\n",
    "u_div_v = divide(u,v) # this returns None anyways, not possible since dividing matrices isn't allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u^{\\;T} * v = \\begin{bmatrix} 71 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v^{\\;T} = \\begin{bmatrix} -4 & 2 & 16 \\\\ -6 & 3 & 24 \\\\ -18 & 9 & 72 \\end{bmatrix} $$  \n",
    "\n",
    "$$ u^{\\;T} * u = \\begin{bmatrix} 94 \\end{bmatrix} $$  \n",
    "\n",
    "$$ \\left \\| u \\right \\|_{2}^{2} = 94.0 $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71]]\n",
      "[[ -4   2  16]\n",
      " [ -6   3  24]\n",
      " [-18   9  72]]\n",
      "[[94]]\n",
      "94.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "u_transpose_v = multiply(transpose(u), v)\n",
    "print(u_transpose_v)\n",
    "\n",
    "u_v_tranpose = multiply(u, transpose(v))\n",
    "print(u_v_tranpose)\n",
    "\n",
    "u_tranpose_u = multiply(transpose(u), u)\n",
    "print(u_tranpose_u)\n",
    "\n",
    "two_norm_u = two_norm(u) ** 2\n",
    "print(two_norm_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix} 1 & 6 & 5\\\\ 0 & -4 & -1\\\\ 7 & 2 & 3 \\end{bmatrix}, \\: \n",
    "B = \\begin{bmatrix} 3 & 1 & 1\\\\ 4 & -1 & 7\\\\ 7 & 0 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A + B = \\begin{bmatrix} 4 & 7 & 6 \\\\ 4 & -5 & 6 \\\\ 14 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A - B = \\begin{bmatrix} -2 & 5 & 4 \\\\ -4 & -3 & -8 \\\\ 0 & 2 & 3 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A * B = \\begin{bmatrix} 62 & -5 & 43 \\\\ -23 & 4 & -28 \\\\ 50 & 5 & 21 \\end{bmatrix} $$  \n",
    "\n",
    "$$ A \\div B = \\text{Not Possible} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7  6]\n",
      " [ 4 -5  6]\n",
      " [14  2  3]]\n",
      "[[-2  5  4]\n",
      " [-4 -3 -8]\n",
      " [ 0  2  3]]\n",
      "[[ 62  -5  43]\n",
      " [-23   4 -28]\n",
      " [ 50   5  21]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "a = np.array([\n",
    "    [1, 6, 5],\n",
    "    [0, -4, -1],\n",
    "    [7, 2, 3]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [3, 1, 1],\n",
    "    [4, -1, 7],\n",
    "    [7, 0, 0]\n",
    "])\n",
    "\n",
    "a_plus_b = add(a,b)\n",
    "print(a_plus_b)\n",
    "\n",
    "a_minus_b = subtract(a,b)\n",
    "print(a_minus_b)\n",
    "\n",
    "a_mult_b = multiply(a,b)\n",
    "print(a_mult_b)\n",
    "\n",
    "a_div_b = divide(a,b)\n",
    "print(a_div_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C = \\begin{bmatrix} 5 & 1 \\\\ -1 & 7 \\\\ 3 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[5, 1],\n",
    "              [-1, 7],\n",
    "              [3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right Pseudo Inverse of C:\n",
    "$$\n",
    "\\begin{align}\n",
    "    &= C^T(C C^T)^{-1}\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.3125 & -0.0078125 & 0.25 \\\\\n",
    "    0.05859375 & 0.14208984 & 0.0390625\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n",
    "\n",
    "Left Pseudo Inverse of C:\n",
    "$$\n",
    "\\begin{align}\n",
    "&= (C^T C)^{-1} C^T \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0.1443299 & -0.02061856 & 0.08591065 \\\\\n",
    "0.0257732 & 0.13917526 & 0.00343643\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3125     -0.0078125   0.25      ]\n",
      " [ 0.05859375  0.14208984  0.0390625 ]]\n",
      "[[ 0.1443299  -0.02061856  0.08591065]\n",
      " [ 0.0257732   0.13917526  0.00343643]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "right_pinv = multiply(transpose(c), np.linalg.inv(multiply(c, transpose(c))))\n",
    "print(right_pinv)\n",
    "\n",
    "left_pinv = multiply(np.linalg.inv(multiply(transpose(c), c)), transpose(c))\n",
    "print(left_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 2: Regression</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ordinary Least Squares below. \n",
    "\n",
    "$$\n",
    "\\hat{\\Theta} = ((X^T \\ X)^{-1} \\ X^T) \\ Y \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** Ordinary Least Squares in terms of what it optimizes.\n",
    "\n",
    "Ordinary Least Squares (OLS) tries to find the line of best fit (in any dimension) in a given dataset. It does this by trying to minimize the distance between the predicted value in the line of best fit and the actual, given value, for each point in the dataset. The goal is to find a $\\hat{\\Theta}$ representing the optimum slopes and intercepts for the function to represent the line of best fit. Here, $\\hat{\\Theta}$ will represent the optimum line of best fit and $\\underline{\\Theta}$ will represent an imperfect line of best fit.\n",
    "\n",
    "In mathematical terms, the for every data point $i$, the predicted value is $x_i^T \\underline{\\Theta}$ and the ground truth value is $y_i$. To this minimization goal, a few modifications are added. The goal is squared to ensure that the magnitude of the error is being minimized. It is multiplied by $\\frac{1}{N}$ to aid in the math when finding the optimal $\\hat{\\Theta}$.\n",
    "\n",
    "Then, summing the minimization goal over all datapoints and creating a function $argmin(\\underline{\\Theta})$ to accept the argument $\\underline{\\Theta}$ and return the optimum $\\hat{\\Theta}$ that minimizes the optimization goal, we have:\n",
    "\n",
    "$$\\hat{\\Theta} = argmin(\\underline{\\Theta})\\Bigg{(} \\ \\frac{1}{N} \\ \\sum_{i}^{N} [X_i^T \\underline{\\Theta} - Y_i]^2\\Bigg{)}$$\n",
    "\n",
    "After doing some calculus, this simplifies to:\n",
    "$$\n",
    "\\hat{\\Theta} = ((X^T \\ X)^{-1} \\ X^T) \\ Y \\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change this cell!\n",
    "# Load in the data about the study on students\n",
    "train = np.loadtxt('train.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = train.T\n",
    "X_train = np.array([x_0, x_1, x_2]).T\n",
    "Y_train = np.expand_dims(y, 1)\n",
    "\n",
    "test = np.loadtxt('test.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = test.T\n",
    "X_test = np.array([x_0, x_1, x_2]).T\n",
    "Y_test = np.expand_dims(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an imaginary study done on 101 students at Crest University. The study surveyed students for the amount they have spent on electronics, books, pencils, and foods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the **amount students spend on electronics ($Y$)** is linearly related to the **amount they spend on books ($ X_{0} $), pencils ($ X_{1}$)**, and **food ($ X_{2}$)**, \n",
    "**implement** the Ordinary Least Squares method to model this regression problem.\n",
    "\n",
    "The data is read in from the previous cell code. **X_train** has the input features, while **Y_train** has corresponding target outputs.\n",
    "\n",
    "After finding a solution, try to measure the error between your predictions and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for training set:  3532.974816901931\n",
      "Error for testing set:  18157.1846126346\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for OLS here. DO NOT use any other libraries in your first implementation.\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "def OLS(X, y):\n",
    "    pseudo_inv = multiply(np.linalg.inv(multiply(X.T, X)), X.T)\n",
    "    theta = multiply(pseudo_inv, y)\n",
    "    return theta\n",
    "\n",
    "# multiply X and theta to get the prediction\n",
    "def get_prediction(X, theta):\n",
    "    assert(X.shape[1] == theta.shape[0])\n",
    "    return multiply(X, theta)\n",
    "\n",
    "# this is the OLS minimization objective as shown above,\n",
    "# (1/n) * sum(...)\n",
    "def get_OLS_error(prediction, ground_truth):\n",
    "    assert(prediction.shape == ground_truth.shape)\n",
    "    \n",
    "    n = prediction.shape[0]\n",
    "    differences = (prediction - ground_truth).flatten() ** 2\n",
    "    return np.sum(differences) / n\n",
    "\n",
    "# perform the regression on the training data\n",
    "theta = OLS(X_train,Y_train)\n",
    "prediction = get_prediction(X_train, theta)\n",
    "error = get_OLS_error(prediction, Y_train)\n",
    "print(\"Error for training set: \", error)\n",
    "\n",
    "# perform the regression on the testing data\n",
    "theta = OLS(X_test, Y_test)\n",
    "prediction = get_prediction(X_test, theta)\n",
    "error = get_OLS_error(prediction, Y_test)\n",
    "print(\"Error for testing set: \", error)\n",
    "\n",
    "\n",
    "# Both errors are pretty large, which shows us that OLS does not\n",
    "# perform well on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what collinearity is.\n",
    "\n",
    "Collinearity is when some of the features in the training set are dependent. This mathematically means that one feature could be represented as a multiple of the other. What this means in the scope of OLS is that there could be a line of solutions rather than a single solution, which defeats the point of regression since a single solution is wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ridge Regression below. \n",
    "\n",
    "Using $\\Phi$ as the feature matrix of $X$, we have:\n",
    "$$\\hat{\\Theta} = (\\Phi^T\\Phi + \\lambda \\ \\text{I})^{-1} \\Phi^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what the purpose ridge regression and its advantages and disadvantages over OLS.\n",
    "\n",
    "Ridge regression is meant to combat the problem of having dependent features produce a line of solutions rather than a single solution. The way it does this is by adding a regulator term in the minimization objective:\n",
    "\n",
    "Ridge regression minimization objective: $\\hat{\\Theta} = argmin(\\underline{\\Theta}) \\frac{1}{2} \\Big{(} || y - \\Phi \\underline{\\Theta}||_2^2 + \\lambda ||\\underline{\\Theta}||_2^2\\Big{)} $\n",
    "\n",
    "Where the $\\lambda ||\\underline{\\Theta}||_2^2$ is the regularization term. The $\\lambda$ is a hyperparameter. Using calculus similar to the math used to derive OLS, we arrive at the equationa above for the optimal $\\hat{\\Theta}$.\n",
    "\n",
    "\n",
    "The advantages of ridge regression over OLS are that since a regularization factor is added to the minimization objective, a single solution is guaranteed. This means that the single, optimal solution was reached rather than one of the many, possibly infinite solutions, that would've seemed like the optimal solution in OLS. Additionally, using ridge regression allows you to examine the data in terms of features rather than the specific numeric values which may provide further insight on the data.\n",
    "\n",
    "The disadvantages of ridge regression is that there is a hyperparameter to tune, $\\lambda$, but since there is only one parameter, it is not too much of a disadvantage. Other than this, there is not really much disadvantage; ridge regression is mostly a step up from OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** your regression model with ridge regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge=0.3\n",
      "Error for training set:  1141.503832815676\n",
      "Error for testing set:  1177.0187352657965\n",
      "\n",
      "ridge=0.1\n",
      "Error for training set:  1141.5038390476188\n",
      "Error for testing set:  1177.0187479999522\n",
      "\n",
      "ridge=0.01\n",
      "Error for training set:  1141.5038457631474\n",
      "Error for testing set:  1177.019314104384\n",
      "\n",
      "ridge=0.03\n",
      "Error for training set:  1141.5038418034155\n",
      "Error for testing set:  1177.0188078334304\n",
      "\n",
      "ridge=0.001\n",
      "Error for training set:  1141.5046272568266\n",
      "Error for testing set:  1177.0762787791\n",
      "\n",
      "ridge=0.003\n",
      "Error for training set:  1141.5039389728722\n",
      "Error for testing set:  1177.0250908037758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Ridge Regression here. DO NOT use any other libraries\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "# analytical solution to RR\n",
    "def RR(X, y, ridge=0.001):\n",
    "    phi_t_phi = multiply(X.T, X)\n",
    "    regularizer = ridge * np.identity(phi_t_phi.shape[0])\n",
    "    phi_t_y = multiply(X.T, y)\n",
    "    inverse_part = np.linalg.inv(add(phi_t_phi, regularizer))\n",
    "    theta = multiply(inverse_part, phi_t_y)\n",
    "    return theta\n",
    "\n",
    "# minimization objective of RR as shown above\n",
    "def get_RR_error(prediction, theta, ground_truth, ridge):\n",
    "    assert(prediction.shape == ground_truth.shape)\n",
    "    \n",
    "    n = prediction.shape[0]\n",
    "    differences = two_norm((ground_truth - prediction).flatten()) ** 2\n",
    "    regularizer = two_norm(theta.flatten()) ** 2\n",
    "    return (differences + regularizer) / 2\n",
    "    \n",
    "\n",
    "# using the get_prediction and get_error functions from above:\n",
    "\n",
    "# check against training data\n",
    "for ridge in [0.3, 0.1, 0.01, 0.03, 0.001, 0.003]:\n",
    "    print(f\"ridge={ridge}\")\n",
    "    theta = RR(X_train,Y_train, ridge)\n",
    "    pred = get_prediction(X_train, theta)\n",
    "    error = get_RR_error(pred, theta, Y_train, ridge)\n",
    "    print(\"Error for training set: \", error)\n",
    "\n",
    "    # test your model on testing data\n",
    "    theta = RR(X_test, Y_test, ridge)\n",
    "    pred = get_prediction(X_test, theta)\n",
    "    error = get_RR_error(pred, theta, Y_test, ridge)\n",
    "    print(\"Error for testing set: \", error)\n",
    "    print()\n",
    "\n",
    "\n",
    "# We can see that the errors here are much smaller than the\n",
    "# error for OLS. additionally, the errors are constant between\n",
    "# the training and testing sets, as well as between different\n",
    "# ridge values, which hint at a more consistent\n",
    "# model less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences ridge regression created for theta compared to OLS, and why these differences even existed. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "Because the error was so much less, we can see that ridge regression performed much better. Because of the advantage RR gives over OLS, the first thought that comes ot mind is that the differences might have been created by collinear features. Indeed, in the dataset, the amount of money students spend on books and pencils may be correlated, leading to RR having a much lower average error.\n",
    "\n",
    "There are differences over each ridge value but since they are so minimal, possibly due to the relative closeness of the chosen ridge values, the errors don't change too much. If the ridge values are set at something like 3000 however, the ridge values alter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other regularizers other than ridge regression, such as LASSO. **Explain** the differences between LASSO and Ridge Regression and how it changes the solution mathematically.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/harish/miniconda3/envs/fri_rl/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/harish/miniconda3/envs/fri_rl/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 70.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=31d38fef64aa7d11ce3688fea2084fdddb0cbc98f1b5b2d66644b1d70ff3c206\n",
      "  Stored in directory: /home/harish/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.1 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for LASSO here\n",
    "# TODO: Plot your regression line over the input points. \n",
    "\n",
    "# Use sklearn.linear_model for the LASSO and Elastic Net implementations\n",
    "!pip install sklearn\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "def LASSO(X, y, ridge=.001):\n",
    "    pass\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = LASSO(X_train,Y_train)\n",
    "\n",
    "# TODO: check against testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the effect elastic nets had on your values for theta compared to OLS. Also try different values for the ridge parameters and describe how they effect your results.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences between LASSO, Ridge Regression and Elastic Nets and how it changes the solution mathematically.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create code for Elastic Nets here. You can use a library such as scipy\n",
    "# TODO: Plot your regression line over the input points.\n",
    "\n",
    "def EN(X,y):\n",
    "    pass\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = EN(X_train,Y_train)\n",
    "\n",
    "# TODO: check against testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of a regularizer?\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give two examples where a regularizer would give more robust models.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain with reference to the dataset why a regularizer achieved better performance than OLS.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature transformation to fit a line to the curve generated from the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('feature_transform.csv', delimiter=',')\n",
    "y = X[:,2].reshape(500, 1)\n",
    "X = X[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Write the lambda function phi which will transform X\n",
    "# TODO: Plot the transformation and the resulting line after transforming\n",
    "\n",
    "phi = \"fill in with lambda function\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,0],y,color=(0.0,0.5,0.0))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what you learned about polynomial regression and explain what is happening to the model as you increase the degrees. Run the cell below and use the slider to help you.\n",
    "\n",
    "**(TODO: Write here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT ALTER\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "data = pd.concat([data,pd.Series(boston.target,name='MEDV')],axis=1)\n",
    "\n",
    "X = data[['LSTAT']].values\n",
    "y = data['MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "temp = pd.DataFrame({'x':x_train.reshape(1, 354)[0], 'y':y_train})\n",
    "temp = temp.sort_values('x')\n",
    "x_train = temp['x'].values.reshape(354,1)\n",
    "y_train = temp['y'].values\n",
    "\n",
    "temp = pd.DataFrame({'x':x_test.reshape(1, 152)[0], 'y':y_test})\n",
    "temp = temp.sort_values('x')\n",
    "x_test = temp['x'].values.reshape(152,1)\n",
    "y_test = temp['y'].values\n",
    "\n",
    "def f(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(x_train,y_train)\n",
    "    y_plot = model.predict(x_test)\n",
    "    \n",
    "    plt.scatter(x_train, y_train, s=10, color='red', alpha=.3)\n",
    "    plt.scatter(x_test, y_test, s=10)\n",
    "\n",
    "    test_sr = (y_test - y_plot)**2\n",
    "    test_ssr = test_sr.sum()\n",
    "    test_asr = test_ssr/len(test_sr)\n",
    "    \n",
    "    y_plot_train = model.predict(x_train)\n",
    "    train_sr = (y_train - y_plot_train)**2\n",
    "    train_ssr = train_sr.sum()\n",
    "    train_asr = train_ssr/len(train_sr)\n",
    "    \n",
    "    plt.plot(x_test, y_plot, label=\"degree %d\" % degree + '; Test Error: %.2f' % test_asr + '; Train Error: %.2f' % train_asr, color='green')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "interact(f, degree = widgets.IntSlider(min=1, max=20, step=1, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
